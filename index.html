<p><link href="style.css" rel="stylesheet"></link></p>
<p><code>Created: 01-30-2021</code><br />
<code>Updated: 03-13-2021</code></p>
<p>hosted @ https://github.com/ericmalekos/RNASeq-walkthough</p>
<h1 id="rna-seq-walkthrough">RNA Seq Walkthrough</h1>
<h2 id="introduction">Introduction</h2>
<p><strong>Purpose:</strong> provide a step-by-step, end-to-end RNA Seq analysis walkthrough.</p>
<p><strong>Environment:</strong> All commands and scripts are made to run on the UCSC <code>courtyard</code> server, but should work in any bash environment.</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">OS:</td>
<td style="text-align: right;">CentOS 7</td>
</tr>
<tr class="even">
<td style="text-align: left;">Python:</td>
<td style="text-align: right;">3.6.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Samtools:</td>
<td style="text-align: right;">1.9</td>
</tr>
</tbody>
</table>
<p>If running on Windows try the <a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">Linux Subsystem for Windows</a> or a Docker instance. Either of these can be used to create Linux environments on Windows (albeit with some drawbacks). I suspect any of the recent Ubuntu releases (16.04, 18.04, 20.04) should work.</p>
<p>If using <code>courtyard</code> (or any remote server) it’s <strong>highly recommended</strong> that you start commands in a <code>screen</code> environment. This is a very simple step and provides at least two substantial advantages:</p>
<ol type="1">
<li>A disruption to your <code>ssh</code> connection will not result in termination of whatever you’re running. (Execution can take many hours to days so this is a likely occurrence)</li>
<li>You can open multiple windows in the same terminal and switch between them, running different commands in each.</li>
</ol>
<p>In the examples below I will always be working in a <code>screen</code> window, however, I will only demonstrate how to start one in section <code>0.1</code>. If your <code>ssh</code> connection is broken use the command <code>screen -r</code> to reconnect.<br />
<a href="https://linuxize.com/post/how-to-use-linux-screen/">More on using screen.</a></p>
<p><strong>The Data</strong>: I will be using paired-end 151 bp Illumina sequence data starting in fastq.gz format.</p>
<p><strong>Note</strong>: <code>fastq.gz</code> files often appear as <code>fq.gz</code>. If this is the case for your data and you’re following this guide, you’ll have to change the scripts to <code>fq.gz</code> wherever <code>fastq.gz</code> appears.</p>
<p><br /></p>
<h2 id="part-0-getting-the-data">Part 0: Getting the Data</h2>
<p>In cases where it seems to make sense I will include the generic command followed by the command I am using for actual data as an example. Otherwise I will just show the command I am using. In the case of generic commands less-than, greater-than markers will be used EX: <code><data/directory></code>.</p>
<h3 id="from-an-sftp-server">0.1 From an SFTP Server</h3>
<ol start="0" type="1">
<li><p>Connect to courtyard, start a <code>screen</code> and navigate to your directory</p>
<pre><code> $ ssh &lt;username&gt;@courtyard.gi.ucsc.edu
 $ ssh emalekos@courtyard.gi.ucsc.edu
 # enter password at prompt

 $ screen
 $ cd &lt;directory/to/transfer/data/to&gt;
 $ cd /public/groups/carpenterlab/people/emalekos/</code></pre></li>
<li><p>Connecting to data storage on remote server</p>
<pre><code> $ sftp &lt;username&gt;@&lt;hostname&gt;
 $ sftp emalekos@sftp.genewiz.com
 # enter password at prompt</code></pre></li>
<li><p>We are now in an <code>sftp</code> environment (<code>$</code> -&gt; <code>sftp&gt;</code>). We want to navigate to the data folder, use <code>ls</code> to list available directories</p>
<pre><code> sftp&gt; ls
 30-422456969

 # Now I enter the folder I found above
 sftp&gt; cd 30-422456969/
 sftp&gt; ls
 00_fastq

 # Found another folder, I&#39;ll enter that.
 sftp&gt; cd 00_fastq/
 sftp&gt; ls -lh
 # This time I find the data files.</code></pre></li>
</ol>
<p>If you are in the data folder you should see something like:</p>
<pre><code>    -rwxr--r--    ? 0        0            2.8G Jan  6 14:04 1M-Ctl-AM_R1_001.fastq.gz
    -rwxr--r--    ? 0        0            3.0G Jan  6 14:04 1M-Ctl-AM_R2_001.fastq.gz
    -rwxr--r--    ? 0        0            3.0G Jan  6 14:04 1M-LPS-AM_R1_001.fastq.gz
    -rwxr--r--    ? 0        0            3.1G Jan  6 14:04 1M-LPS-AM_R2_001.fastq.gz
    -rwxr--r--    ? 0        0            3.0G Jan  6 14:04 1W-Ctl-AM_R1_001.fastq.gz
    -rwxr--r--    ? 0        0            3.3G Jan  6 14:13 1W-Ctl-AM_R2_001.fastq.gz
    -rwxr--r--    ? 0        0            3.0G Jan  6 14:13 1W-LPS-AM_R1_001.fastq.gz
    -rwxr--r--    ? 0        0            3.2G Jan  6 14:13 1W-LPS-AM_R2_001.fastq.gz
    -rwxr--r--    ? 0        0            2.1G Jan  6 14:14 2M-CSE-AM_R1_001.fastq.gz
    -rwxr--r--    ? 0        0            2.3G Jan  6 14:14 2M-CSE-AM_R2_001.fastq.gz</code></pre>
<ol type="1">
<li><p>To copy the files from this server to your workspace use the <code>get</code> command. Here I give some examples</p>
<pre><code> # To copy a single file
 sftp&gt; get 1M-Ctl-AM_R1_001.fastq.gz

 # To copy all files
 sftp&gt; get *

 # To copy all files ending in &#39;fastq.gz&#39;
 sftp&gt; get *fastq.gz

 # To copy all files with &#39;LPS&#39; somewhere in the middle
 sftp&gt; get *LPS*

 # To copy all files starting with &#39;LPS&#39; and with &#39;_R1&#39; somewhere before the end
 sftp&gt; get LPS*_R1*</code></pre></li>
</ol>
<p><strong>NOTE:</strong> ’<code>*</code>’ performs wildcard expansion - it can fill in for any characters and is useful for pattern matching. When you use it it’s good to check which patterns it’s actually matching by double tapping the <code>Tab</code> key. This will list everything that matches. If you see the files you want press <code>Enter</code> to execute.</p>
<p><br /></p>
<h3 id="from-google-drive">0.2 From Google Drive</h3>
<p>You can use this method to transfer data files from your Google Drive account to <code>courtyard</code>. It works but is somewhat clunky, and there may be a better way. For instance this method transfers one file at a time, if all relevant files could be zipped together only one command would be required.</p>
<ol type="1">
<li><p>We will use the python package <code>gdown</code>. The first time you use this you will have to install it</p>
<pre><code> pip3 install gdown --user</code></pre></li>
<li>Get the share link for your Gdrive file
<ul>
<li><strong>IMPORTANT</strong> - make sure the the file is accessible to “Anyone with the link”</li>
<li>The relevant part of the link is the string of characters between <code>/d/</code> and <code>/view/</code></li>
<li>In the example below this is what we want: <code>1gc0Nfl693O49BECq34g6zno-ThJdBqw_</code></li>
</ul>
<p><img src="./Images/0_gdrive.png" /></p></li>
<li>Run with python3
<ul>
<li><p>Now you can start a python3 session and copy the files over</p>
<pre><code>  $ python3

  # Now in python shell  
  &gt;&gt;&gt; import gdown
  &gt;&gt;&gt; url=&quot;https://drive.google.com/uc?id=1gc0Nfl693O49BECq34g6zno-ThJdBqw_&quot;
  &gt;&gt;&gt; output=&quot;desired_filename.fastq.gz&quot;
  &gt;&gt;&gt; gdown.download(url, output, quiet=False)</code></pre></li>
<li><p>Alternatively you can open a text editor and write a script like this one (or copy the script from the gihub page).</p>
<pre><code>  import gdown
  url_prefix = &quot;https://drive.google.com/uc?id=&quot;
  suffix = &quot;.fastq.gz&quot;
  filedict = {&quot;file_1_1&quot; : &quot;1gc0Nfl693O49BECq34g6zno-ThJdBqw_&quot;,
              &quot;file_1_2&quot; : &quot;&lt;file 1_2 share link&gt;&quot;,
              &quot;file_2_1&quot; : &quot;&lt;file 2_1 share link&gt;&quot;,
              &quot;file_2_2&quot; : &quot;&lt;file 2_2 share link&gt;&quot;}

  for key, value in filedict.items():
      gdown.download(url_prefix + value, key + suffix, quiet=False)</code></pre>
<p>After changing the entries in <code>filedict</code> to your desired filenames and corresponding links, run with:</p>
<pre><code>  python3 get_Gdrive.py</code></pre></li>
</ul></li>
</ol>
<p><br /></p>
<h3 id="make-smaller-files-for-pipeline-practice-optional">0.3 Make Smaller Files for Pipeline Practice (OPTIONAL)</h3>
<p>You may want to practice running through the pipeline with a reduced dataset. This would allow you to troubleshoot much more quickly than if you tried processing all of your data at once. We can make some reduced, but still functional <code>fastq.gz</code> files with the following command</p>
<pre><code>    zcat &lt;file.in&gt; | head -n &lt;# of lines&gt; | gzip &gt; &lt;file.out&gt;

    zcat full_file.fastq.gz | head -n 10000000 | gzip &gt; reduced_file.gz</code></pre>
<p>This example takes the first <code>10000000</code> lines of the input file (or the first <code>2500000</code> fastq entries). The resulting gzipped file is ~200 MB. Adjust <code>&lt;# of lines&gt;</code> as you see fit, but make it divisible by 4 to avoid cutting off fastq entries.</p>
<p><br /></p>
<h2 id="part-1-quality-control">Part 1: Quality Control</h2>
<p>Before going any further I’m going to organize my workspace.</p>
<pre><code>    # move reads to a new directory
    $ mkdir raw_reads
    $ mv *.gz raw_reads</code></pre>
<h3 id="read-quality-with-fastqc">1.1 Read Quality with FastQC</h3>
<p>FastQC seems to be the standard read quality checking tool. For each read file it generates an HTML file containing its findings. <a href="https://linuxize.com/post/how-to-use-linux-screen/">More on FastQC.</a></p>
<pre><code>    # Download and unpack FastQC

    $ wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.9.zip
    $ unzip fastqc_v0.11.9.zip
    $ chmod +x FastQC/*
    $ rm fastqc_v0.11.9.zip

    # Make an output directory and run FastQC on all fastq.gz files

    $ mkdir quality_raw_reads
    $ for i in raw_reads/*fastq.gz; do FastQC/fastqc $i -o quality_raw_reads/ ; done</code></pre>
<h3 id="viewing-the-fastqc-results">1.2 Viewing the FastQC Results</h3>
<p>Now we want to view the HTML pages in a web browser. I don’t know of an easy way to do this while the files are on the server. One option is to use <code>sftp get</code> as described above to move the files to your computer.</p>
<p>Here’s an alternative that does not require downloading the files, but instead temporarily mounts the <code>quality_raw_reads</code> to your computer using <code>sshfs</code>. These commands are all run on your local computer, not <code>courtyard</code></p>
<pre><code>    # Install sshfs on *your* computer if you don&#39;t have it
    $ sudo apt install sshfs
    # enter password

    # make local mount directory and mount files to it
    $ mkdir mount

    $ sshfs &lt;username&gt;@courtyard.gi.ucsc.edu:&lt;path/to/files&gt; &lt;/mount/to&gt;
    $ sshfs emalekos@courtyard.gi.ucsc.edu:/public/groups/shariatilab/emalekos/quality_raw_reads/ ./mount
    # Enter courtyard password

    # open a mounted file in your browser
    &lt;browser&gt; &lt;mount/file.html&gt;
    $ google-chrome mount/A01_1_fastqc.html</code></pre>
<p>Here are my reads’ quality scores for the first <code>fastq.gz</code> file.</p>
<p><img src="./Images/1_read_raw_example.png" /></p>
<p>And here is the adapter content</p>
<p><img src="./Images/1_adapter_raw_example.png" width="900" height="600" /></p>
<p>After viewing, unmount the files:</p>
<pre><code>    $ fusermount -u &lt;mounted_dir&gt;
    $ fusermount -u mount</code></pre>
<h3 id="trimming-and-adapter-removal">1.3 Trimming and Adapter Removal</h3>
<p>Trimming low quality bases in Illumina reads is a common step in sequence alignment pipelines. However, modern aligners including <em>HISAT2, BWA-MEM and STAR</em> (which we will use in the next section) perform “soft clipping” which eliminates the need for additional trimming. Using trimming tools in a way that is insensitive will likely reduce the mapping rate and can distort results.<br />
What about adapter removal? The author of <em>STAR</em> suggests it could be useful when <a href="https://github.com/alexdobin/STAR/issues/455">aligning short reads</a> and the author of <em>BWA</em> suggests it <a href="https://sourceforge.net/p/bio-bwa/mailman/bio-bwa-help/thread/530E1378.3040008%40cam.ac.uk/">should be done</a>.</p>
<p>Good tools for adapter trimming are Trimmomatic, Cutadapt and NGmerge. Here I use NGmerge which determines the adapter sequences without user input which is nice. However, unlike the other two, it only works for paired-end reads.</p>
<pre><code>    # get NGmerge
    $ git clone https://github.com/jsh58/NGmerge
    $ cd NGmerge
    $ make
    $ cd ..

    # make new directory for reads with adapters removed
    $ mkdir adapter_trimmed_reads</code></pre>
<p>To run <em>NGmerge</em> on all of the <code>fastq.gz</code> files in <code>raw_reads/</code> you can copy this script.</p>
<pre><code>    # Make file in nano text editor
    $ nano ngmerge_adapters.sh

    # Copy what&#39;s below with Ctrl + c and paste into nano file with Ctrl + Shift + v

    #!/usr/bin/env bash
    readDir=raw_reads/
    minRead=31
    threads=8
    maxQ=41
    outDir=adapter_trimmed_reads/
    outName=trim
    for i in ${readDir}*_1.fastq.gz
    do
            prefix=$(basename $i _1.fastq.gz)
            echo $prefix

            ./NGmerge/NGmerge \
                    -a \
                    -z \
                    -v \
                    -e $minRead \
                    -u $maxQ \
                    -n $threads \
                    -1 ${readDir}${prefix}_1.fq.gz \
                    -2 ${readDir}${prefix}_2.fq.gz \
                    -o ${outDir}${prefix}_${outName}
    done</code></pre>
<p>Regarding the variables above the <code>for</code> statement<br />
- If you have different directory names for reads and output you will need to change them.<br />
- <code>minRead</code> is based on the FastQC adapter output which shows the adapters ending around base 31. Update this based on your FastQC results.<br />
- <code>maxQ</code> is set to 41 because in Illumina &gt;=1.8 the top quality score is 41 rather than 40.<br />
- <code>threads</code> should be chosen with regard to the other jobs running on the server. See below for neighborly thread # setting</p>
<p>Once you’ve adjusted the variables, run the script:</p>
<pre><code>    # Ctrl+o then Enter to save
    # Ctrl+x to exit nano

    # make script executable
    $ chmod +x ngmerge_adapters.sh

    # run script
    $ ./ngmerge_adapters.sh</code></pre>
<p>This took a few hours when I ran it with the above settings on 10 sets of paired-end reads.</p>
<h3 id="on-setting-threads">1.4 On Setting Threads</h3>
<p>In the previous step, and going forward, we are going to be making use of a <em>threads</em> option in many of the tools we run. <em>threads</em> is short for “threads of execution” which, in this context, refers to a program performing some sort of data processing. In general a program uses a single thread of execution, but most bioinformatics tools allow the user to specify the number of threads. This is useful because the larger the number of threads, the more data can be processed in parallel, and the faster the operation can complete. However every computer has only a finite number of <em>threads</em>, probably between 4 and 16 on your personal computer, and 64 on <code>courtyard</code>. The <em>64 threads</em> are shared among all <code>courtyard</code> users so the number available to you will certainly be less than that. To check the current availability use:</p>
<pre><code>    $ top

    # use Ctrl + c to exit top</code></pre>
<p>You’ll see something like:</p>
<p><img src="./Images/1_top.png" /></p>
<p>I’ve cropped the image so that it only shows one running process - my <em>BWA</em> run - but you’ll see all of the current processes. The most important thing to note for the thread discussion is the <strong>%Cpu(s)</strong> number. Here it’s at <strong>25.5%</strong> which means <em>~ 64 * 0.25 = 16</em> threads are in use, and 48 are available (also, under my <em>BWA</em> instance the <strong>%CPU</strong> is <strong>1197%</strong>, where 100% is equivalent to a single thread, so I must have run this with 12 threads). The main thing is to be considerate when choosing threads and <strong>NOT</strong> contribute to a situation where <em>%Cpu(s) becomes &gt;= 100%</em>. This would add significant overhead as the server switches among processes and slow everyone’s computation down.</p>
<h3 id="qc-again">1.5 QC Again</h3>
<p>Here we repeat Parts 1.1 &amp; 1.2, updating the path to point to the new QC files.</p>
<pre><code>    $ mkdir quality_trimmed
    $ for i in adapter_trimmed_reads/*fastq.gz; do FastQC/fastqc $i -o quality_trimmed/ ; done

    $ sshfs emalekos@courtyard.gi.ucsc.edu:/public/groups/shariatilab/emalekos/quality_raw_reads/ ./mount

    $ google-chrome mount/A01_trim_1_fastqc.html</code></pre>
<p><img src="./Images/1_adapters_removed.png" width="900" height="600" /></p>
<pre><code>    $ fusermount -u mount</code></pre>
<h2 id="part-2-mapping-and-counting">Part 2: Mapping and Counting</h2>
<p>When it comes to counting genes and transcripts there are two approaches: 1. Align reads to genome + counting 2. “Pseudo align” reads to transcriptome + quantification</p>
<p>The first (implemented in programs like <em>STAR</em>, <em>cufflinks</em> and generally anything that produces a BAM/SAM file) has been the standard.<br />
<strong>Pros:</strong><br />
- produces BAM/SAM for downstream analysis<br />
- often have additional features like marking novel splice junctions<br />
<strong>Cons:</strong><br />
- relatively slow and/or memory hungry<br />
- Although fast by aligner standards, <em>STAR</em> requires ~32 GB of RAM when mapping to human genome - pseudo aligners on the other hand are often remarked to be runnable on a laptop - maybe not as accurate as the pseudo aligners (although, of course, this doesn’t seem to be straightforward)</p>
<p>The second (implemented in <em>Salmon</em> and <em>Kallisto</em>) is a newer method that relies on “pseudo-alignment” and seems to be increasingly popular. It compares reads to the transcriptome rather than the genome.<br />
<strong>Pros:</strong><br />
- much less computationally demanding<br />
- may give better results in terms of both gene and transcript level quantification<br />
- <em>Salmon</em> and <em>Kallisto</em> are in remarkable agreement, uncommon for bioinformatics tools<br />
- made to deal with multimapping reads from the outset (I know <em>STAR</em> also has parameters that can be set in this regard, not sure about other aligners).<br />
<strong>Cons:</strong><br />
- requires an annotated transcriptome<br />
- less compatible with many downstream tools - in particular those that require BAM/SAMs</p>
<h3 id="part-2.2-star-alignment-and-counting">Part 2.2 STAR Alignment and Counting</h3>
<p><em>STAR</em> is a fast but memory hungry aligner (recommends at least 32 GBs for human genome) with a built in gene counting function yeilding the same results as <em>htseq-count</em> (default settings) but saving a step by counting as it maps. It seems to be among the most popular transcript aligning tools. <a href="https://github.com/alexdobin/STAR">More on <em>STAR</em></a>. Before mapping we need to build a genome index. We need:<br />
1. <em>STAR</em><br />
2. Annotation files (GTF or GFF)<br />
3. Genome</p>
<p>My data is from mouse, the files can be found <a href="https://www.gencodegenes.org/mouse/release_M25.html">here</a>. Note that the STAR documentation recommends using the <em>primary_assembly</em> (PRI) files.</p>
<pre><code>    # Make new directory and get STAR
    mkdir STAR
    cd STAR

    wget https://github.com/alexdobin/STAR/archive/2.7.7a.zip
    unzip 2.7.7a.zip

    cd STAR-2.7.7a/source
    make

    cd ../..

    # Annotation and Genome
    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.primary_assembly.annotation.gtf.gz

    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/GRCm38.primary_assembly.genome.fa.gz

    # Files need to be unzipped
    gunzip *.gz</code></pre>
<p>Now we can create an index of the genome.</p>
<pre><code>    mkdir M25_index

    STAR-2.7.7a/source/STAR\
            --runThreadN 8 \
            --runMode genomeGenerate \
            --genomeDir ./M25_index \
            --genomeFastaFiles ./GRCm38.primary_assembly.genome.fa \
            --sjdbGTFfile ./gencode.vM25.primary_assembly.annotation.gtf \
            --sjdbOverhang 150</code></pre>
<p>The arguments in this step are pretty straightforward. You might change <code>–sjdbOverhang</code> based on your read lengths. The optimum choice is <em>length of longest read - 1</em> Now we perform the alignment and gene counting in <em>STAR</em>.<br />
<strong>NOTE</strong>: The script provided will cycle through all of the pair end reads in the <code>readDir</code>. Currently it loads and unloads the genome into memory for every iteration of the loop which is inneficient. There is a way to load the genome once before entering the loop and unload it after but I was having trouble getting it to work.</p>
<pre><code>    # First make some output directories
    for i in &lt;reads_directory&gt;/*&lt;file_suffix&gt;; do mkdir $(basename $i &lt;file_suffix&gt;) ; done
    for i in ../adapter_trimmed_reads/*_1.fastq.gz; do mkdir $(basename $i _1.fastq.gz) ; done

    ls
    # You should now see directories corresponding to each pair of reads

    # I call STAR from the script &#39;star_map_sort.sh&#39;:
    nano star_map_sort.sh

    threads=12
    readDir=../adapter_trimmed_reads/
    index=./M25_index/
    outDir=./

    for read in ${readDir}*_1.fastq.gz
    do
            base=&quot;$(basename &quot;$read&quot; _1.fastq.gz)&quot;
            name1=&quot;$base&quot;_1.fastq.gz   
            name2=&quot;$base&quot;_2.fastq.gz
            echo
            echo First Read:  $name1
            echo Second Read: $name2

            outDir=&quot;$base&quot;/
            echo Output Directory: $outDir

            STAR-2.7.7a/source/STAR \
            --genomeDir $index \
            --readFilesIn \
            ${readDir}/${name1} \
            ${readDir}/${name2} \
            --readFilesCommand zcat \
            --outSAMtype BAM SortedByCoordinate \
            --outSAMattributes NH HI AS nM XS \
            --twopassMode Basic \
            --outFilterMultimapNmax 10 \
            --quantMode GeneCounts \
            --runThreadN $threads \
            --alignEndsType Local \
            --outFileNamePrefix ${outDir}/${base}
    done

    # save and exit: CTRL + o, ENTER, CTRL + x

    chmod +x star_map_sort.sh

    ./star_map_sort.sh</code></pre>
<p>The <code>echo</code> statements should give you some indication that you’re pointing to the intended files/directories. Use <code>CTRL + c</code> to cancel the run if you’re pointing to the wrong locations.</p>
<p>Arguments:<br />
- <code>readFilesCommand</code>: set to <code>zcat</code> if files are in <code>.gz</code> format, otherwise remove this.<br />
- <code>outSAMtype</code>: sorted BAM file.<br />
- <code>outSAMattributes</code>: attributes to include in alignment file. <code>NH HI AS nM</code> are standard. I include <code>XS</code> because I believe it is required for <em>juncBASE</em>.<br />
- <code>twopassMode</code>: default is single pass, <code>Basic</code> indicates to 2-pass mapping. I understand this to be preferred for considering splice junctions.<br />
- <code>outFilterMultimapNmax</code>: maximum number of loci the read is allowed to map to. 10 is default. reads that map to &gt;10 loci are not included in BAM Alignment file.<br />
- <code>quantMode</code>: <code>GeneCounts</code> results in output file containing gene counts the same as <em>htseq-count</em> with default settings. All library combinations are accounted for (unstranded, first/second stranded) each as a separate column, so you need to pick the correct column from this output. See <em>STAR</em> manual/<em>htseq</em> manual for more info.<br />
- <code>runThreadN</code>: number of threads.<br />
- <code>alignEndsType</code>: alignmnent. <code>Local</code> is default and performs soft clipping.<br />
- <code>outFileNamePrefix</code>: there will be many output files. It’s probably best to send output from each run to its own directory. This assigns the prefix to each.</p>
<h3 id="part-2.3-salmon-pseudo-alignment-and-quantification">Part 2.3 Salmon Pseudo-Alignment and Quantification</h3>
<p>As with <em>STAR</em> we will build an index. We need:<br />
1. <em>Salmon</em><br />
2. An annotated transcriptome<br />
3. The primary genome assembly</p>
<pre><code>    mkdir Salmon
    cd Salmon

    wget https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz
    tar -xvf salmon-1.4.0_linux_x86_64.tar.gz

    rm *.gz

    # rename directory as &quot;salmon&quot;
    mv salmon-latest_linux_x86_64/ salmon

    #Download transcriptome and genome
    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/gencode.vM25.transcripts.fa.gz

    wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M25/GRCm38.primary_assembly.genome.fa.gz</code></pre>
<p>I copied the following code from <a href="https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/">here</a></p>
<pre><code>    # Make a decoy file
    grep &quot;^&gt;&quot; &lt;(gunzip -c GRCm38.primary_assembly.genome.fa.gz) | cut -d &quot; &quot; -f 1 &gt; decoys.txt

    sed -i.bak -e &#39;s/&gt;//g&#39; decoys.txt

    # Create a new file combining transcriptome and genome, with transcriptome first
    cat gencode.vM25.transcripts.fa.gz GRCm38.primary_assembly.genome.fa.gz &gt; gentrome.fa.gz

    salmon/bin/salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode</code></pre>
<p>Arguments:<br />
- <code>t</code>: transcriptome and genome concatenated file with which to build index<br />
- <code>d</code>: decoy file used to avoid spurious mappings. I don’t understand this very well, there’s a discussion of why this is important <a href="https://www.biostars.org/p/335414/">here</a><br />
- <code>p</code>: number of threads<br />
- <code>i</code>: name of the generated index<br />
- <code>gencode</code>: tell <em>Salmon</em> the input is in Gencode format</p>
<p>For more options and explanations use <code>salmon/bin/salmon index –help</code></p>
<p>You may see a bunch of warning messages. Check the dialogue <a href="https://github.com/COMBINE-lab/salmon/issues/214">here</a> for an explanation. You may want to set the <code>–keepDuplicates</code> flag.</p>
<p>Now we are ready to quantify our reads.</p>
<pre><code>    $ nano salmon_quant.sh

    threads=12
    readDir=../adapter_trimmed_reads/
    index=./salmon_index/
    outDir=./

    for read in ${readDir}*_1.fastq.gz
    do
            base=&quot;$(basename &quot;$read&quot; _1.fastq.gz)&quot;
            name1=&quot;$base&quot;_1.fastq.gz
            name2=&quot;$base&quot;_2.fastq.gz
            echo
            echo First Read:  $name1
            echo Second Read: $name2

            outDir=&quot;$base&quot;/
            echo Output Directory: $outDir

            salmon/bin/salmon quant \
            --libType A \
            --index $index \
            --mates1 ${readDir}/${name1} \
            --mates2 ${readDir}/${name2} \
            --threads $threads \
            --softclip \
            --output ${outDir}/$&quot;base&quot;

    done

    # save and exit: CTRL + o, ENTER, CTRL + x

    chmod +x salmon_quant.sh

    ./salmon_quant.sh</code></pre>
<p>Arguments:<br />
- <code>libType</code>: A autodetects library strandedness<br />
- <code>index</code>: points to the output of “salmon index …”<br />
- <code>mates1/2</code>: points to pair of reads<br />
- <code>threads</code>: number of threads<br />
- <code>softclip</code>: allow soft clipping.</p>
<p>As with <em>STAR</em> there are many other parameters that can be tweaked. Use <code>salmon quant –help-reads</code> to view them.</p>
